{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de sentimiento\n",
    "\n",
    "¿Recuerdas qué es el NLP (Procesamiento de Lengauje Natural)? Hace tiempo, cuando estudiamos *Feature Selection*, vimos que no podíamos alimentar unmodelo directamente con datos que no fueran numéricos, por lo que si teníamos textos había que ingeniárselas para traducir esa información de modo que \"la entienda\" nuestro modelo.\n",
    "\n",
    "A continuación, vamos a repasar lo que vimos en su día y profundizaremos en su aplicación final, donde utilizaremos alguno de los modelos que ya controlamos. Con esto, podrás enlazar los puntos que quedaban sueltos.\n",
    "\n",
    "Centrándonos en el problema que vamos a utilizar de referencia, partiremos de dos documentos de texto con opiniones de películas en IMDB. Tendremos un documento con frases positivas y otro con frases negativas. Evidentemente, necesitaremos esta clasificación para que nuestro modelo pueda aprender. En este caso, para facilitarnos las cosas, el ratio de frases positivas y negativas será aproximadamente el mismo.\n",
    "\n",
    "### Importamos librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargando los datos\n",
    "\n",
    "A diferencia de otros ejemplos que hemos visto con anterioridad, no tendremos un dataset como tal, sino que tendremos unos ficheros de texto con frases que deberemos tratar. Además, ya que lo tenemos así, utilizaremos esta estructura de datos (listas) para realizar el análisis que, dado que es un iterable, nos servirá del mismo modo que si fuera un Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_positivo = []\n",
    "for line in open('data/train_pos.txt', 'r', encoding='latin1'):\n",
    "    reviews_train_positivo.append(line.strip())\n",
    "    \n",
    "reviews_train_negativo = []\n",
    "for line in open('data/train_neg.txt', 'r', encoding='latin1'):\n",
    "    reviews_train_negativo.append(line.strip())\n",
    "    \n",
    "reviews_test = []\n",
    "for line in open('data/test.txt', 'r', encoding='latin1'):\n",
    "    reviews_test.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juntamos las reviews de train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = reviews_train_positivo + reviews_train_negativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos creamos la lista del ``target`` en base a los datos que hemos leído:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = [1 if i < len(reviews_train_positivo) else 0 for i in range(len(reviews_train_positivo) + len(reviews_train_negativo))]\n",
    "\n",
    "# Y los datos de test, que serán 50/50:\n",
    "target_test = [1 if i < len(reviews_test)/2 else 0 for i in range(len(reviews_test))]\n",
    "\n",
    "# Y nos creamos el target como unión de los dos, ya que estarán en orden:\n",
    "target = target_train + target_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y nos aseguramos leyendo un par de frases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is not the typical Mel Brooks film. It was much less slapstick than most of his movies and actually had a plot that was followable. Leslie Ann Warren made the movie, she is such a fantastic, under-rated actress. There were some moments that could have been fleshed out a bit more, and some scenes that could probably have been cut to make the room to do so, but all in all, this is worth the price to rent and see it. The acting was good overall, Brooks himself did a good job without his characteristic speaking to directly to the audience. Again, Warren was the best actor in the movie, but \"Fume\" and \"Sailor\" both played their parts well.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I thought that My Favorite Martian was very boring and drawn out!! It was not funny at all. The audience just sat through the whole movie and didn't laugh at all!!! Not even the kids laughed!! That is sad for a Disney movie!! I thought they could have found somebody better to play the martian rather than Christopher Lloyd!! He was really stupid!! And he was not funny!! I thought the talking suit was really dumb!!! In the original television series the suit doesn't talk and move around!! In my opinion they should not have wasted their time on this movie!! I give it two thumbes down!! Really a waste of time and I would not recommend the movie to anybody!!! Thank You!!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train[20001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedes observar, la primera opinión es de las buenas y la segunda de las malas.\n",
    "\n",
    "\n",
    "### Pretratamiento de textos\n",
    "\n",
    "\n",
    "Por otra parte, si queremos analizar estas frases, no podemos utilizarlas directamente (bueno sí que podríamos pero no sería nuestra mejor opción). Si queremos sacar el máximo partido a nuestros datos de texto, tendremos que hacer un pretratamiento de datos, como vimos hace tiempo.\n",
    "\n",
    "¿Y este tratamiento en qué consiste? Pues si recuerdas bien, el primer paso consistía en limpiar todo aquello que no sea puro texto, como pueden ser caracteres de puntuación o marcas de texto (estamos ante una extracción HTML).\n",
    "\n",
    "Por lo tanto, vamos a eliminar todos estos caracteres raros que no aportan información, lo cual podremos hacer de diferentes modos. En este caso, lo que haremos será eliminar los signos de puntuación (como ``,``, ``.``, ``?``...) y sustituir las etiquetas HTML y los caracteres separadores de palabras (como ``-`` o ``/``) por espacios. Además, hemos visto que la distinción entre mayúsculas y minúsculas también nos puede jugar malas pasadas, por lo que lo convertiremos todo a minúsculas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    \n",
    "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "reviews_train_clean = preprocess_reviews(reviews_train)\n",
    "reviews_test_clean = preprocess_reviews(reviews_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el antes y el después de la frase positiva que habíamos leído:\n",
    "\n",
    "**Antes**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is not the typical Mel Brooks film. It was much less slapstick than most of his movies and actually had a plot that was followable. Leslie Ann Warren made the movie, she is such a fantastic, under-rated actress. There were some moments that could have been fleshed out a bit more, and some scenes that could probably have been cut to make the room to do so, but all in all, this is worth the price to rent and see it. The acting was good overall, Brooks himself did a good job without his characteristic speaking to directly to the audience. Again, Warren was the best actor in the movie, but \"Fume\" and \"Sailor\" both played their parts well.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Después**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is not the typical mel brooks film it was much less slapstick than most of his movies and actually had a plot that was followable leslie ann warren made the movie she is such a fantastic under rated actress there were some moments that could have been fleshed out a bit more and some scenes that could probably have been cut to make the room to do so but all in all this is worth the price to rent and see it the acting was good overall brooks himself did a good job without his characteristic speaking to directly to the audience again warren was the best actor in the movie but fume and sailor both played their parts well'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train_clean[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorización\n",
    "\n",
    "Bien, ya hemos conseguido extraer la información de la que podremos extraer patrones para poder crear nuestro modelo. Sin embargo, nuestro modleo no sabe cómo tratar estos datos de manera directa, no entiende los ``strings``. Para que entienda la información, hay que convertirla a datos numéricos, a lo que hemos llamado *vectorización*.\n",
    "\n",
    "La forma más sencilla de hacer esto es crear una matriz gigante donde llevemos el recuento de cada una de las palabras que aparecen en todos los textos, aplicado a cada frase. Es decir, crearnos una columna por cada palabra única en la suma de todas y cada una de las frases de nuestro conjunto de frases de entrenamiento, y rellenar cada registro (cada frase) haciendo las cuentas por frase.\n",
    "\n",
    "Para esto podemos utilizar el método ``CountVectorizer``, que nos dejará la opción de hacer el conteo o simplemente registrar si tiene esa palabra (1) o no (0), lo cual podemos establecer con el parámetro ``binary``.\n",
    "\n",
    "*NOTA*: Este método nos devolverá una matriz *sparse*, que si bien recuerdas era una matriz que tenía una forma especial para ocupar menos espacio en memoria, pero que para tratarlas había que convertirlas con ``toarray()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "baseline_vectorizer = CountVectorizer(binary=False)\n",
    "baseline_vectorizer.fit(reviews_train_clean)\n",
    "\n",
    "X_baseline = baseline_vectorizer.transform(reviews_train_clean)\n",
    "X_test_baseline = baseline_vectorizer.transform(reviews_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n"
     ]
    }
   ],
   "source": [
    "print(X_baseline.shape)\n",
    "# baseline_vectorizer.vocabulary_ # ordenado alfabéticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Para obtener solo las palabras o nombres de las columnas que vamos a utilizar:\n",
    "# baseline_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenando el modelo\n",
    "\n",
    "Utilizaremos un modelo de regresión lineal para crer nuestro detector de sarcasmo, a ver qué tal se nos da. En este caso, parece que es una buena idea, ya que los modelos lineales:\n",
    "* Son fáciles de interpretar (podremos sacar fácilmente la importancia de cada n_grama\n",
    "* Debido a su naturaleza tan \"simple\" puede aprender mucho más rápido que otros algoritmos\n",
    "* Trabajan bien con matrices sparse\n",
    "\n",
    "Para probar el modelo, haremos un poco de GridSearchCV, que es algo que nos encanta. En este caso, como parámetros a modificar no tendremos muchas opciones, por lo que solamente variaremos ``C``, probando [0.01, 0.05, 0.25, 0.5, 1] para encontrar el valor que mejor accuracy nos devuelva:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Final Accuracy: 0.88072\n",
      "Best params: {'model__C': 0.01}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=Pipeline(steps=[('model', LogisticRegression())]),\n",
       "             n_jobs=-1, param_grid={'model__C': [0.01, 0.05, 0.25, 0.5, 1]},\n",
       "             scoring='accuracy', verbose=4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "def train_model(X_TRAIN, X_TEST, TARGET_TRAIN, TARGET_TEST, model=LogisticRegression()):\n",
    "    \n",
    "    # Creamos pipeline:\n",
    "    pipe = Pipeline([('model', model)])\n",
    "    \n",
    "    # Creamos parámetros:\n",
    "    params = {\n",
    "        'model__C': [0.01, 0.05, 0.25, 0.5, 1]\n",
    "    }\n",
    "    \n",
    "    # Creamos grid:\n",
    "    grid = GridSearchCV(pipe,\n",
    "                        params,\n",
    "                        cv=5,\n",
    "                        n_jobs=-1,\n",
    "                        scoring='accuracy',\n",
    "                        verbose=4\n",
    "                       )\n",
    "    \n",
    "    # Entrenamos:\n",
    "    grid.fit(X_TRAIN, TARGET_TRAIN)\n",
    "    \n",
    "    # Y predecimos:\n",
    "    print (f\"Final Accuracy: {accuracy_score(TARGET_TEST, grid.best_estimator_.predict(X_TEST))}\")\n",
    "    print (f\"Best params: {grid.best_params_}\")\n",
    "    return grid\n",
    "    \n",
    "train_model(X_baseline, X_test_baseline, target_train, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminando Stop Words\n",
    "\n",
    "¿Qué eran las *stop words*? Si no lo recuerdas, las *stop words* eran aquellas palabras tan comunes que aparecen tantas veces que no nos aportan información, como ``if``, ``but``, ``we``, ``he`` o ``she``. Normalmente, eliminar estas palabras no cambia el sentido semántico de un texto y SUELE dar mejores resultados. Esto será mucho más útil cuanto mayores sean las secuencias de palabras (n-grams).\n",
    "\n",
    "Así que, ahora, vamos a aplicar el ``CountVectorizer`` tras eliminar las *stop words* que vienen por defecto en el paquete ``nltk``, que son las más comunes, aunque podríamos utilizar otras propias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TheBridge\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stop_words = stopwords.words('english')\n",
    "def remove_stop_words(corpus):\n",
    "    removed_stop_words = []\n",
    "    for review in corpus:\n",
    "        \n",
    "        # Para cada review elimina las stopwords, y separa todas las palabras por espacio\n",
    "        removed_stop_words.append(\n",
    "            ' '.join([word for word in review.split() \n",
    "                      if word not in english_stop_words])\n",
    "        )\n",
    "    return removed_stop_words\n",
    "\n",
    "no_stop_words_train = remove_stop_words(reviews_train_clean)\n",
    "no_stop_words_test = remove_stop_words(reviews_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(no_stop_words_train)\n",
    "\n",
    "X = cv.transform(no_stop_words_train)\n",
    "X_test = cv.transform(no_stop_words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 87046)\n",
      "Stop words eliminadas: 17\n"
     ]
    }
   ],
   "source": [
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Stop words eliminadas:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Final Accuracy: 0.87976\n",
      "Best params: {'model__C': 0.05}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=Pipeline(steps=[('model', LogisticRegression())]),\n",
       "             n_jobs=-1, param_grid={'model__C': [0.01, 0.05, 0.25, 0.5, 1]},\n",
       "             scoring='accuracy', verbose=4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(binary=True,\n",
    "                    stop_words=english_stop_words)\n",
    "\n",
    "cv.fit(reviews_train_clean)\n",
    "\n",
    "X = cv.transform(reviews_train_clean)\n",
    "X_test = cv.transform(reviews_test_clean)\n",
    "\n",
    "train_model(X, X_test, target_train, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 86918)\n",
      "Stop words eliminadas: 145\n"
     ]
    }
   ],
   "source": [
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Stop words eliminadas:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTA:* Como hemos mencionado, eliminar las *stop words* SUELE (no siempre) mejorar el modelo. En este caso, como puedes comprobar, resulta que nos reduce el *performance* del modelo, por lo que no será buena idea utilizarlo en este en concreto.\n",
    "\n",
    "Si te has fijado, hemos utilizado el parámetro ``stop_words`` para elegir la lista de *stop words* que queremos eliminar. En este caso ha sido la lista por defecto, pero podríamos utilizar otra que hayamos analizado que sea mejor, como podría ser ['in', 'of', 'at', 'a', 'the'] ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro paso típico que se suele utilizar en este tipo de problemas consiste en normalizar las palabras del corpus mediante la extracción del significado base de cada una de ellas. Para esto, utilizaremos dos técnicas: *Stemming* y *Lematización*.\n",
    "\n",
    "## Stemming\n",
    "\n",
    "El Stemming es considerado una técnica de normalización por aproximación de fuerza bruta, aunque no por ello significa que sea peor. Hay varios algoritmos, pero por lo general se usarán reglas básicas para cortar las terminaciones de las palabras.\n",
    "\n",
    "NLTK tiene varias implmentaciones de los algoritmos de *Stemming*:\n",
    "* PorterStemmer\n",
    "* SnowballStemmer\n",
    "\n",
    "A continuación podemos ver un ejemplo de la implementación de cada uno de ellos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli die mule deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']\n",
    "\n",
    "singles = [stemmer.stem(i) for i in plurals]\n",
    "\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli die mule deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedes observar, los resultadso son muy parecidos, y es que ambos algoritmos son similares, donde podemos ver SnowballStemmer como una versión mejorada, un poco más agresiva que el PorterStemmer (más clásico y ligeramente más conservador).\n",
    "\n",
    "En este caso, utilizaremos el PorterStemmer por ser más conservador: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stemmed_text(corpus):\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "stemmed_reviews_train = get_stemmed_text(reviews_train_clean)\n",
    "stemmed_reviews_test = get_stemmed_text(reviews_test_clean)\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(stemmed_reviews_train)\n",
    "X_stem = cv.transform(stemmed_reviews_train)\n",
    "X_test = cv.transform(stemmed_reviews_test)\n",
    "\n",
    "train_model(X_stem, X_test, target_train, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_baseline.shape)\n",
    "print(X_stem.shape)\n",
    "print(\"Diff X normal y X tras stemmer y vectorización:\", X_baseline.shape[1] - X_stem.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lematización\n",
    "\n",
    "La técnica de *Lematización* trabaja identificando la parte gramatical de una palabra dada, aplicando tras ello reglas más complejas para transformar la palabra en su verdadera raíz.\n",
    "\n",
    "Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para castellano deberíamos instalar la siguiente librería:\n",
    "# pip install es-lemmatizer   para castellano\n",
    "\n",
    "# Para inglés:\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'studies',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']\n",
    "singles = [lemmatizer.lemmatize(plural) for plural in plurals]\n",
    "\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien antes no observábamos apenas diferencia entre PorterStemmer y SnowballStemmer, ahora sí que se observan resultados diferentes. No hay un método mucho mejor que otro, simplemente son diferentes, por lo que no podemos decir que siempre va a ser mejor utilizar Lematización frente a Stemmer, o viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmatized_text(corpus):\n",
    "    \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "# Lematizamos las reviews\n",
    "lemmatized_reviews_train = get_lemmatized_text(reviews_train_clean)\n",
    "lemmatized_reviews_test = get_lemmatized_text(reviews_test_clean)\n",
    "\n",
    "# Vectorizamos con conteo tras lematizar\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(lemmatized_reviews_train)\n",
    "\n",
    "X = cv.transform(lemmatized_reviews_train)\n",
    "X_test = cv.transform(lemmatized_reviews_test)\n",
    "\n",
    "train_model(X, X_test, target_train, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Diff X normal y X tras lematizador y vectorización:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gramas\n",
    "\n",
    "Existe una manera mediante la cual podemos agregar más poder predictivo a nuestro modelo: agregar secuencias de palabras, generalmente de 2 o 3 palabras (bigramas o trigramas).\n",
    "\n",
    "Por ejemplo, si una reseña tuviera la secuencia de 3 palabras ``didn't love movie``, solo consideraríamos estas palabras individualmente con un modelo unigrama, dejando sin capturar el significado real de la frase, pues no enlazaríamos ``didn't`` con ``love``, que tiene una connotación negativa, sino que estaríamos viendo la palabra ``love`` por ahí, lo que tiene connotaciones positivas.\n",
    "\n",
    "Gracias a scikit-learn esto se vuelve realmente fácil (como todo lo que llevamos visto hasta ahora). Solamente utilizando el argumento ``ngram_range`` con cualquiera de las clases ``Vectorizer`` nos valdrá:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = 'this is foo bar'\n",
    "sentence2 = 'this'\n",
    "sentence3 = 'foo bar'\n",
    "\n",
    "two = ngrams(sentence.split(),2)\n",
    "three = ngrams(sentence.split(),3)\n",
    "\n",
    "for grams in two:\n",
    "    print(grams)\n",
    "\n",
    "print('###############')\n",
    "for grams in three:\n",
    "    print(grams)\n",
    "\n",
    "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1,3))\n",
    "\n",
    "vector = ngram_vectorizer.fit_transform([sentence,sentence2, sentence3]).toarray()\n",
    "print(vector)\n",
    "print(len(vector[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podremos extaer el vocabulario (como antes):\n",
    "ngram_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probarlo sobre nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_vectorizer = CountVectorizer(binary=True,\n",
    "                                  ngram_range=(1,2))\n",
    "\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "train_model(X, X_test, target_train, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Diff X normal y X tras lematizador y vectorización:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vaya, parece que ahora sí que hemos mejorado el *performance* del modelo, ¡¡y con menos variables!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Otra técnica muy extendida que también habíamos visto en el pasado era la técnica TF-IDF (Term Frequency-Inverse Document Frequency), que consiste en un estadístico para cada palabra al que se le asigna un factor de ponderación que podemos usar en lugar de representaciones binarias o de conteo de palabras.\n",
    "\n",
    "Hay varias formas de realizar la transformación TF-IDF, pero, resumiendo,  se trata de una técnica que tiene como objetivo representar la cantidad de veces que una palabra determinada aparece en un documento (una opinión de una película, en nuestro caso) en relación a la cantidad de documentos en el corpus en el que aparece la palabra, donde las palabras que aparecen en muchos documentos tienen un valor más cercano a cero y las palabras que aparecen en menos documentos tienen valores más cercanos a 1.\n",
    "\n",
    "Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ln(N+1/df+1) +1\n",
    "\n",
    "sent1 = 'My name is Ralph'\n",
    "sent2 = 'Ralph is fat'\n",
    "sent3 = 'Ralph'\n",
    "\n",
    "test = TfidfVectorizer()\n",
    "test.fit_transform([sent1, sent2, sent3])\n",
    "print(test.idf_)\n",
    "print(test.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos ahora sobre nuestro dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_vectorizer.fit(reviews_train_clean)\n",
    "\n",
    "X = tfidf_vectorizer.transform(reviews_train_clean)\n",
    "X_test = tfidf_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "\n",
    "train_model(X, X_test, target_train, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)\n",
    "\n",
    "Hasta ahora, hemos probado estas técnicas con un modelo LogisticRegression, que es un modelo lineal. Por lo general, este tipo de problemas con *sparse datasets*, es decir, con conjuntos de datos con muchos 0 (como lo que tenemos nosotros,), suelen responder mejor ante modelos lineales, ya que podemos particularizr de mejor manea el efecto de cada palabra, además de entrenar relativamente más rápido que con otros modelos más complejos.\n",
    "\n",
    "Por lo tanto, si estamos hablando de algoritmos lineales, podríamos probar otro algoritmo lineal, como es el SVC con kernel lineal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# SVM con bigramas:\n",
    "ngram_vectorizer = CountVectorizer(binary=True,\n",
    "                                   ngram_range=(1, 2))\n",
    "\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "train_model(X, X_test, target_train, target_test, model=LinearSVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versión final\n",
    "\n",
    "Tras diferentes iteraciones, se ha llegado a que los mejores resultados se obtienen con la eliminación de un pequeño conjunto de *stop words*, junto con un rango de n-gramas de 1 a 3 y un SVC lineal, tal como se muestra a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "stop_words = ['in', 'of', 'at', 'a', 'the']\n",
    "\n",
    "ngram_vectorizer = CountVectorizer(binary=True,\n",
    "                                   ngram_range=(1, 3),\n",
    "                                   stop_words = stop_words)\n",
    "\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "\n",
    "train_model(X, X_test, target_train, target_test, model=LinearSVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Palabras: positivas vs. negativas\n",
    "\n",
    "Con el modelo lineal, podemos obtener la importancia de cada variable en base al valor de sus coeficientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(reviews_train_clean)\n",
    "X = cv.transform(reviews_train_clean)\n",
    "\n",
    "log_reg = LogisticRegression(C=0.5)\n",
    "log_reg.fit(X, target)\n",
    "\n",
    "#print(log_reg.coef_)\n",
    "#print(cv.get_feature_names())\n",
    "\n",
    "# Montamos un diccionario con palabra -> coeficiente\n",
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(cv.get_feature_names(), log_reg.coef_[0])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TOP Positivas:\")\n",
    "\n",
    "for best_positive in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1], \n",
    "    reverse=True)[:5]:\n",
    "    print(best_positive)\n",
    "    \n",
    "print('---------------------------')\n",
    "print(\"TOP Negativas:\")\n",
    "\n",
    "for best_negative in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1])[:5]:\n",
    "    print(best_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
